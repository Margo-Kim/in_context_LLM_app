{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp askDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Assisted Data Analysis Notebook\n",
    "This notebook is designed for AI-assisted data labeling and querying using OpenAI's APIs. It demonstrates modular programming practices, AI-assisted labeling, and dataset querying functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import requests\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: This part sets up the notebook for use with nbdev, a system that turns Jupyter Notebooks into Python modules. It imports necessary libraries: requests for making HTTP requests (e.g., to the OpenAI API), openai for interacting with OpenAI's GPT model, and pandas for data manipulation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ask Dataset Class\n",
    "\n",
    "- Initialization (__init__): Accepts a data file, attempts to read it as a CSV using Pandas, and stores it in an instance variable.\n",
    "\n",
    "- dataset_string Method: Converts the DataFrame (from the CSV file) to a string.\n",
    "\n",
    "- ask_chatgpt Method: Sends a query to the OpenAI ChatGPT model, along with the dataset, and returns the model's response. This is the core functionality, utilizing the OpenAI API to analyze the dataset and answer questions about it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class AskDataset:\n",
    "    def __init__(self, data_file):\n",
    "        try:\n",
    "            # Attempt to read the uploaded file\n",
    "            data_csv = pd.read_csv(data_file)\n",
    "            self.data_csv = data_csv  # Store the DataFrame for later use\n",
    "        except Exception as e:\n",
    "            # Provide detailed error information\n",
    "            raise Exception(f\"Failed to initialize AskDataset with file: {data_file}. Error: {e}\")\n",
    "\n",
    "    def dataset_string(self):\n",
    "        # Convert the DataFrame to a string and return\n",
    "        return self.data_csv.to_string(index=False)\n",
    "\n",
    "    def ask_chatgpt(self, question):\n",
    "        try:\n",
    "           \n",
    "            # API endpoint for ChatGPT\n",
    "            url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "            # Headers including the Authorization with your API key\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "            }\n",
    "\n",
    "            # Data payload for the request\n",
    "            data_payload = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a problem solver. Go through the dataset to find the answer.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Based on {self.dataset_string()} solve the Question: {question}\"}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            response = requests.post(url, headers=headers, json=data_payload, timeout=10)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            response_json = response.json()\n",
    "\n",
    "            if 'choices' in response_json and response_json['choices']:\n",
    "                message = response_json['choices'][0]['message']\n",
    "                if isinstance(message, str):\n",
    "                    return message.strip()\n",
    "                elif isinstance(message, dict) and 'content' in message:\n",
    "                    # Extracting content if the message is a dictionary\n",
    "                    return message['content'].strip()\n",
    "                else:\n",
    "                    return \"Received unexpected response type: \" + str(message)\n",
    "            else:\n",
    "                return \"No choices in response or empty response: \" + str(response_json)\n",
    "\n",
    "        except requests.exceptions.Timeout:\n",
    "            return \"Request timed out. Please try again.\"\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            return f\"HTTP error occurred: {err}\"\n",
    "        except Exception as e:\n",
    "            return f\"An unexpected error occurred: {e}\"\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usuage Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To solve the question, we need to count the number of rows that have NaN values in the \"Label\" column. Based on the provided conversation, the rows with NaN values are:\n",
      "\n",
      "1. Rows with index [0, 3, 4, 6, 10, 12, 14, 16, 20, 22, 23, 24, 28, 29, 31, 33, 34, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 49, 50]\n",
      "\n",
      "There are a total of 30 rows with NaN values in the \"Label\" column.\n"
     ]
    }
   ],
   "source": [
    "ask_dataset = AskDataset('/workspaces/ai-assisted-coding_panther/231-3.csv')\n",
    "\n",
    "response = ask_dataset.ask_chatgpt('how many rows are NaN?')\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation**: In this example, the AskDataset class is instantiated with a CSV file containing data. Then, a question is posed to the ChatGPT model about this data. The expected output is an answer from ChatGPT, providing insights based on the data.\n",
    "\n",
    "This approach demonstrates how the AskDataset class can be used to leverage AI (via OpenAI's GPT model) to analyze and extract insights from a dataset without manually writing data analysis code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
