{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inLoop\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Assisted Data Analysis Notebook\n",
    "This notebook is designed for AI-assisted data labeling and querying using OpenAI's APIs. It demonstrates modular programming practices, AI-assisted labeling, and dataset querying functionalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import pandas as pd\n",
    "import requests \n",
    "import openai\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inLoop Class Explanation\n",
    "\n",
    "## Overview of `inLoop` Class\n",
    "- **Class Name**: `inLoop`\n",
    "- **Purpose**: To process a dataset and use an AI model to generate labels for the data.\n",
    "\n",
    "## `__init__(self, data_file)`\n",
    "- **Functionality**: Initializes the class instance with a dataset.\n",
    "- **Parameters**: \n",
    "  - `data_file`: The path to the dataset file.\n",
    "- **Internal Variables**:\n",
    "  - `self.data_csv`: Stores the data file for use in other methods.\n",
    "\n",
    "## `get_prediction_for_text(self, data)`\n",
    "- **Functionality**: Sends a text data piece to the OpenAI API and retrieves a predicted label.\n",
    "- **Parameters**: \n",
    "  - `data`: A string containing the text data for which a label is to be predicted.\n",
    "- **OpenAI API Interaction**:\n",
    "  - Constructs a request to the OpenAI API with a specific instruction to return labels separated by commas, with no explanations.\n",
    "  - Parses the API response to extract the label.\n",
    "\n",
    "## `turn_dataframe(self, prediction, data_chunk)`\n",
    "- **Functionality**: Converts predictions and text data into a pandas DataFrame.\n",
    "- **Parameters**: \n",
    "  - `prediction`: A string of comma-separated labels returned by the AI model.\n",
    "  - `data_chunk`: A pandas DataFrame chunk of the dataset.\n",
    "- **Process**: \n",
    "  - Splits the prediction string into individual labels.\n",
    "  - Pads the labels and text data to ensure equal length.\n",
    "  - Creates a new DataFrame with text and corresponding labels.\n",
    "\n",
    "## `update_labels(self, start_index, revised_labels)`\n",
    "- **Functionality**: Updates the labels in the original dataset.\n",
    "- **Parameters**: \n",
    "  - `start_index`: The index in the dataset from where labels should be updated.\n",
    "  - `revised_labels`: A list of new labels to replace the old ones.\n",
    "- **Process**: Iterates over the revised labels and updates them in the dataset at the corresponding index.\n",
    "\n",
    "## `get_updated_data(self)`\n",
    "- **Functionality**: Retrieves the updated dataset.\n",
    "- **Returns**: The updated dataset with new labels.\n",
    "\n",
    "\n",
    "## In-the-Loop Process\n",
    "The `inLoop` class is crucial for an in-the-loop labeling system. It interacts with an AI model to initially label data and provides functionalities to update these labels based on user feedback or further analysis. This iterative process allows for continuous improvement of data labeling quality and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class inLoop:\n",
    "\n",
    "    def __init__(self, data_file):\n",
    "        self.data_csv = data_file\n",
    "\n",
    "    \n",
    "    def get_predictions(self, prompt):\n",
    "        predictions = []\n",
    "        for _, row in data_chunk.iterrows():\n",
    "            text = row['Text']\n",
    "            prediction = self.get_prediction_for_text(text)  # Implement this method\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    def get_prediction_for_text(self, data):\n",
    "        try:\n",
    "            # Set your OpenAI API key\n",
    "            key = \"\"\n",
    "\n",
    "            # API endpoint for ChatGPT\n",
    "            url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "            # Headers including the Authorization with your API key\n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "            }\n",
    "             \n",
    "            # Data payload for the request\n",
    "            data_payload = {\n",
    "                \"model\": \"gpt-3.5-turbo\",\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a labeler and have to generate 5 labels. Each line has to have a label which is OTR(Opportunity to Respond), PRS(Praise),REP(Reprimand), NEU(None of the Above). And the result should only return the 5 labels and it should be seperated in comma. DO NOT INCLUDE ANY EXPLANATIONS. JUST LABELS SEPERATED IN COMMAS. ONLY GENERATE LABELS(OTR, PRS, REP, NEU) FOR EACH ROW. NO EXPLANATIONS. \"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Based on {data} labels: \"}\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # Make the POST request\n",
    "            response = requests.post(url, headers=headers, json=data_payload)\n",
    "            response_json = response.json()\n",
    "\n",
    "            # Check if 'choices' is in the response and if it's not empty\n",
    "            if 'choices' in response_json and response_json['choices']:\n",
    "                # Accessing the first choice's message\n",
    "                message = response_json['choices'][0]['message']['content']\n",
    "                # Ensure the message is a string before calling strip()\n",
    "                if isinstance(message, str):\n",
    "                    return message.strip()\n",
    "                else:\n",
    "                    # Handle the case where message is not a string\n",
    "                    return \"Received non-string response: \" + str(message)\n",
    "            else:\n",
    "                return \"No choices in response or empty response: \" + str(response_json)\n",
    "\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    \n",
    "    def turn_dataframe(self, prediction, data_chunk):\n",
    "        predicted_list = [item.strip() for item in prediction.split(',')]\n",
    "        print(predicted_list)\n",
    "        text_variable = data_chunk['Text'].to_list()\n",
    "        print(text_variable)\n",
    "\n",
    "        if len(predicted_list) < 5:\n",
    "            predicted_list = predicted_list + ([None] * (5 - len(predicted_list)))\n",
    "        if len(data_chunk) < 5 :\n",
    "            text_variable = text_variable + ([None] * (5 - len(data_chunk)))\n",
    "        df = pd.DataFrame({'Text': text_variable, 'Label' : predicted_list})\n",
    "        print(df)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def update_labels(self, start_index, revised_labels):\n",
    "        for i, label in enumerate(revised_labels):\n",
    "            self.data_csv.at[start_index + i, 'Label'] = label\n",
    "\n",
    "    def get_updated_data(self):\n",
    "        return self.data_csv\n",
    "\n",
    "    # def get_predictions(self, prompt):\n",
    "    #     try:\n",
    "    #         # Set your OpenAI API key\n",
    "    #         key = \"\"\n",
    "\n",
    "    #         # API endpoint for ChatGPT\n",
    "    #         url = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "    #         # Headers including the Authorization with your API key\n",
    "    #         headers = {\n",
    "    #             \"Content-Type\": \"application/json\",\n",
    "    #             \"Authorization\": f\"Bearer {openai_api_key}\"\n",
    "    #         }\n",
    "\n",
    "    #         # Data payload for the request\n",
    "    #         data_payload = {\n",
    "    #             \"model\": \"gpt-3.5-turbo\",\n",
    "    #             \"messages\": [\n",
    "    #                 {\"role\": \"system\", \"content\": \"prompt\"},\n",
    "    #                 {\"role\": \"user\", \"content\": f\"Based on {self.dataset_string} solve the Question: {prompt}\"}\n",
    "    #             ]\n",
    "    #         }\n",
    "\n",
    "    #         # Make the POST request\n",
    "    #         response = requests.post(url, headers=headers, json=data_payload)\n",
    "    #         response_json = response.json()\n",
    "\n",
    "    #         # Check if 'choices' is in the response and if it's not empty\n",
    "    #         if 'choices' in response_json and response_json['choices']:\n",
    "    #             # Accessing the first choice's message\n",
    "    #             message = response_json['choices'][0]['message']\n",
    "    #             # Ensure the message is a string before calling strip()\n",
    "    #             if isinstance(message, str):\n",
    "    #                 return message.strip()\n",
    "    #             else:\n",
    "    #                 # Handle the case where message is not a string\n",
    "    #                 return \"Received non-string response: \" + str(message)\n",
    "    #         else:\n",
    "    #             return \"No choices in response or empty response: \" + str(response_json)\n",
    "\n",
    "    #     except Exception as e:\n",
    "    #         return f\"Error: {e}\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data_chunk = pd.read_csv('/workspaces/ai-assisted-coding_panther/test_ai.csv')\n",
    "\n",
    "in_loop_instance = inLoop(data_chunk)\n",
    "\n",
    "# Define the chunk size\n",
    "chunk_size = 5\n",
    "\n",
    "# Process the dataset in chunks\n",
    "for start_index in range(0, len(data_chunk), chunk_size):\n",
    "    # Selecting a chunk of rows\n",
    "    chunk = data_chunk.iloc[start_index:start_index + chunk_size]\n",
    "\n",
    "    # Concatenating the text entries in the chunk\n",
    "    concatenated_text = ' '.join(chunk['Text'].fillna(''))\n",
    "\n",
    "    # Getting predictions for the concatenated text\n",
    "    prediction = in_loop_instance.get_prediction_for_text(concatenated_text)\n",
    "\n",
    "    # Split the prediction into individual labels\n",
    "    predicted_labels = prediction.split(',') if prediction else ['None'] * chunk_size\n",
    "\n",
    "    # Update the labels in the original dataset\n",
    "    in_loop_instance.update_labels(start_index, predicted_labels[:len(chunk)])\n",
    "\n",
    "# Fetching the updated dataset\n",
    "updated_dataset = in_loop_instance.get_updated_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Timestamp                                               Text Label\n",
      "0      0:00  Good morning class, today we are going to lear...   OTR\n",
      "1   00:04.4         Can anyone tell me what a civilization is?   NEU\n",
      "2       NaN  Yes, that's right. A civilization is a complex...   NEU\n",
      "3       NaN  Now, let's talk about the first civilization w...   NEU\n",
      "4       NaN  They lived in a region called Mesopotamia. Can...   NEU\n",
      "5   00:22.8  Good job, Sarah! Mesopotamia is between the Ti...   PRS\n",
      "6   00:27.0  The Sumerians invented many things we still us...   NEU\n",
      "7       NaN              Excellent, they did invent the wheel!   PRS\n",
      "8   00:35.9  They also invented writing. They wrote on clay...   PRS\n",
      "9       NaN     Jack, please stop talking while I am teaching.   REP\n"
     ]
    }
   ],
   "source": [
    "print(updated_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "1. Loading the Dataset:\n",
    "\n",
    "The dataset is loaded from a CSV file into a pandas DataFrame. This step involves reading the file located at /workspaces/ai-assisted-coding_panther/test_ai.csv. The DataFrame, data_chunk, contains the data that will be processed.\n",
    "\n",
    "2. Initializing the inLoop Instance:\n",
    "\n",
    "An instance of the inLoop class is created with the loaded DataFrame. This instance, in_loop_instance, will be used to interact with the AI model for label predictions and to update the dataset.\n",
    "\n",
    "3. Defining Chunk Size:\n",
    "\n",
    "The chunk_size variable is set to 5. This determines the number of rows from the dataset to be processed in each iteration. The idea is to process and label the dataset in smaller, manageable batches.\n",
    "\n",
    "4. Processing the Dataset in Chunks:\n",
    "\n",
    "The dataset is processed in chunks, each consisting of 5 rows (as defined by chunk_size). For each chunk:\n",
    "a. Chunk Selection: A subset of the dataset, chunk, is selected based on the current start_index. This subset contains 5 rows of text data.\n",
    "b. Text Concatenation: The text entries in the chunk are concatenated into a single string, concatenated_text. This is done to prepare the data for label prediction. Missing values (NaN) are filled with an empty string to ensure consistency.\n",
    "c. Getting Predictions: The concatenated text is sent to the get_prediction_for_text method of the inLoop instance. This method interacts with the AI model to get a prediction string, which contains labels separated by commas.\n",
    "d. Label Processing: The prediction string is split into individual labels. If a prediction is not available, a default label 'None' is used for each entry in the chunk.\n",
    "e. Updating the Dataset: The original dataset is updated with these new labels using the update_labels method. This method places the labels at the correct positions in the dataset, starting from start_index.\n",
    "\n",
    "5. Fetching the Updated Dataset:\n",
    "\n",
    "After processing all chunks, the updated dataset with new labels is retrieved using the get_updated_data method. This dataset, updated_dataset, now contains the original text data along with the AI-generated labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
